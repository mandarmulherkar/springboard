---
title: "ag_fico_clustering_copy.Rmd"
author: "Mandar Mulherkar"
date: "November 10, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# This mini-project is based on the K-Means exercise from 'R in Action'
# Go here for the original blog post and solutions
# http://www.r-bloggers.com/k-means-clustering-from-r-in-action/

# Exercise 0: Install these packages if you don't have them already

# http://marcoghislanzoni.com/blog/2014/08/29/solved-installing-rattle-r-3-1-mac-os-x-10-9/

install.packages("RGtk2")
install.packages("rattle")
install.packages(c("cluster", "rattle","NbClust"))
library("RGtk2")
library("rattle")
library("cluster")
library("ggplot2")

# Now load the data and look at the first few rows
#ag.fico.groups = read.csv("/Users/Mandar/Documents/datascience/springboard_ag_capstone/clustering/91_people_clean.csv", header = TRUE)
ag.fico.groups = read.csv("/Users/Mandar/Documents/datascience/springboard_ag_capstone/clustering/91_mp_people_sample_1.csv", header = TRUE)
#ag.fico.groups = read.csv("/Users/Mandar/Documents/datascience/springboard_ag_capstone/clustering/fico_income_20k_clustering_data_clean.csv", header = TRUE)

#library(dplyr)
#ag.people = read.csv("mp_people_properties_header_consol.csv", header = TRUE)
#joined <- left_join(ag.fico.groups, ag.people, by = c("distinct_id" = "distinct_id"), suffix = c("1", "2"))
#write.csv(joined, file = "fico_people.csv")
  
# Create the required subset
data <- ag.fico.groups[,c(1:15)]
dim(data)
str(data)

#data <- subset(data, fico_group == 3 | fico_group == 4 | fico_group == 5 | fico_group == 2)
data <- subset(data, select=c("fico_group", "number_of_makes_selected", "number_of_models_selected", "number_of_trims_selected", "number_of_dealers_selected", "sessions"))
dim(data)

data$employment_status <- as.numeric(data$employment_status)
data$state <- as.numeric(data$state)

# Exercise 1: Remove the first column from the data and scale
# it using the scale() function
str(data)
head(data)
g <- ggplot(data, aes(fico_group))
g + geom_bar()
```

```{r}
f <- ggplot(data, aes(fico_group, number_of_makes_selected, col = fico_group))
f + geom_jitter()

f <- ggplot(data, aes(fico_group, number_of_models_selected, col = fico_group))
f + geom_jitter()

f <- ggplot(data, aes(fico_group, number_of_dealers_selected, col = fico_group))
f + geom_jitter()

f <- ggplot(data, aes(fico_group, sessions, col = fico_group))
f + geom_jitter()
```


```{r}

# Bootstrap 95% CI for R-Squared
library(boot)
# function to obtain R-Squared from the data 
rsq <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample 
  fit <- lm(formula, data=d)
  return(summary(fit)$r.square)
} 
# bootstrapping with 1000 replications 
#results <- boot(data=data, statistic=rsq, 
#                R=1000, formula=fico_group~number_of_makes_selected+number_of_models_selected+number_of_trims_confirmed+number_of_dealers_confirmed)

str(data)
results <- boot(data=data, statistic=rsq, 
                R=1000, formula=fico_group~sessions)

library(ggplot2)
View(data)

g <- ggplot(data, aes(fico_group))
g + geom_bar()

# view results
str(results$data)
str(results)
plot(results)

# get 95% confidence interval 
boot.ci(results, type="bca")

data <- results$data

dim(data)
```


```{r}
df <- scale(data[-1])

# Now we'd like to cluster the data using K-Means. 
# How do we decide how many clusters to use if you don't know that already?
# We'll try two methods.

# Method 1: A plot of the total within-groups sums of squares against the 
# number of clusters in a K-means solution can be helpful. A bend in the 
# graph can suggest the appropriate number of clusters. 

# https://rpubs.com/gabrielmartos/ClusterAnalysis

wssplot <- function(data, nc=15, seed=12345){
	              wss <- (nrow(data)-1)*sum(apply(data,2,var))
               	      for (i in 2:nc){
		        set.seed(seed)
	                wss[i] <- sum(kmeans(data, centers=i)$withinss)}
	                
		      plot(1:nc, wss, type="b", xlab="Number of Clusters",
	                        ylab="Within groups sum of squares")
	   }

wssplot(df)
```

```{r}
# Exercise 2:
#   * How many clusters does this method suggest?
#   * Why does this method work? What's the intuition behind it?
#   * Look at the code for wssplot() and figure out how it works

# Method 2: Use the NbClust library, which runs many experiments
# and gives a distribution of potential number of clusters.

library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=3, max.nc=10, method="kmeans")
table(nc$Best.n[1,])

barplot(table(nc$Best.n[1,]),
	          xlab="Number of Clusters", ylab="Number of Criteria",
		            main="Number of Clusters Chosen by 26 Criteria")
```


```{r}
# Exercise 3: How many clusters does this method suggest?


# Exercise 4: Once you've picked the number of clusters, run k-means 
# using this number of clusters. Output the result of calling kmeans()
# into a variable fit.km

set.seed(4321)
fit.km <- kmeans(df, 3, nstart=25)
fit.km$size
```

```{r}
# Now we want to evaluate how well this clustering does.
fit.km$centers
fit.km$cluster
```

```{r}
# Exercise 5: using the table() function, show how the clusters in fit.km$clusters
# compares to the actual wine types in wine$Type. Would you consider this a good
# clustering?
aggregate(data[-1], by=list(cluster=fit.km$cluster), mean)
ct.km <- table(data$fico_group, fit.km$cluster)
ct.km 
```

```{r}
library(flexclust)
randIndex(ct.km)
# Exercise 6:
# * Visualize these clusters using  function clusplot() from the cluster library
# * Would you consider this a good clustering?

clusplot(df, fit.km$cluster, main='2D representation of the Cluster solution', color = TRUE, shade = TRUE, labels=2, lines=0)
```


```{r}
library(ggplot2)
plot(data[c("fico_group", "number_of_models_selected")], col = fit.km$cluster)

fit.km$cluster
f <- ggplot(data, aes(fico_group, number_of_makes_selected))
f + geom_jitter(col = fit.km$cluster)
```







```{r}
### Try the above for approved or declined.

# Now load the data and look at the first few rows
ag.decision.groups = read.csv("/Users/Mandar/Documents/datascience/springboard_ag_capstone/clustering/91_mp_people_decision_binary.csv", header = TRUE)

#library(dplyr)
#ag.people = read.csv("mp_people_properties_header_consol.csv", header = TRUE)
#joined <- left_join(ag.fico.groups, ag.people, by = c("distinct_id" = "distinct_id"), suffix = c("1", "2"))
#write.csv(joined, file = "fico_people.csv")

# Create the required subset
datadecision <- ag.decision.groups[,c(1:19)]
datadecision <- na.omit(datadecision)

datadecision$state <- as.numeric(datadecision$state)
datadecision$employment_status <- as.numeric(datadecision$employment_status)
```


```{r}
str(datadecision)
head(datadecision)
sapply(datadecision, class)
dim(datadecision)

g <- ggplot(datadecision, aes(fico_group))
g + geom_bar()
```

```{r}
str(datadecision)
datasub <- subset(datadecision, number_of_models_selected < 100)
f <- ggplot(datasub, aes(sessions, number_of_models_selected, fill = factor(fico_group)))
f + geom_jitter(aes(col = factor(fico_group)))

f <- ggplot(datadecision, aes(number_of_calculator_clicks, fill = factor(fico_group)))
f + geom_bar()

f <- ggplot(datadecision, aes(income, age, col = fico_group))
f + geom_jitter()

f <- ggplot(datadecision, aes(fico_group, sessions, col = age_group))
f + geom_jitter()

f <- ggplot(datadecision, aes(fico_group, sessions, col = fico_group))
f + geom_jitter()
```



```{r}
# Exercise 1: Remove the first column from the data and scale
# it using the scale() function
df1 <- scale(datadecision[-1])

# Now we'd like to cluster the data using K-Means. 
# How do we decide how many clusters to use if you don't know that already?
# We'll try two methods.

# Method 1: A plot of the total within-groups sums of squares against the 
# number of clusters in a K-means solution can be helpful. A bend in the 
# graph can suggest the appropriate number of clusters. 

wssplot <- function(datadecision, nc=15, seed=123) {
  wss <- (nrow(datadecision)-1)*sum(apply(datadecision,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(datadecision, centers=i)$withinss)}
  
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}

wssplot(df1)
```


```{r}
# Exercise 2:
#   * How many clusters does this method suggest?
#   * Why does this method work? What's the intuition behind it?
#   * Look at the code for wssplot() and figure out how it works

# Method 2: Use the NbClust library, which runs many experiments
# and gives a distribution of potential number of clusters.

library(NbClust)
set.seed(1234)
nc <- NbClust(df1, min.nc=2, max.nc=9, method="kmeans")
table(nc$Best.n[1,])

barplot(table(nc$Best.n[1,]),
        xlab="Number of Clusters", ylab="Number of Criteria",
        main="Number of Clusters Chosen by Criteria")
```


```{r}
# Exercise 3: How many clusters does this method suggest?

# Exercise 4: Once you've picked the number of clusters, run k-means 
# using this number of clusters. Output the result of calling kmeans()
# into a variable fit.km
set.seed(43210)
fit.km1 <- kmeans(df1, 2, nstart=25)
fit.km1$size
```

```{r}
# Exercise 5: using the table() function, show how the clusters in fit.km$clusters
# compares to the actual wine types in wine$Type. Would you consider this a good
# clustering?
# Now we want to evaluate how well this clustering does.
fit.km1$centers 
fit.km1$cluster
```

```{r}
aggregate(datadecision[-1], by=list(cluster=fit.km1$cluster), mean)

ct.km1 <- table(datadecision$decision_status, fit.km1$cluster)
ct.km1  
```

```{r}
library(flexclust)
randIndex(ct.km1)
# Exercise 6:
# * Visualize these clusters using  function clusplot() from the cluster library
# * Would you consider this a good clustering?

clusplot(df1, fit.km1$cluster, color = TRUE, shade = TRUE)
fit.km1
```








```{r}
### Checking for decision status clustering.
```


```{r}
### Try the above for approved or declined.

# Now load the data and look at the first few rows
ag.decision.groups = read.csv("/Users/Mandar/Documents/datascience/springboard_ag_capstone/clustering/91_mp_people_decision_binary.csv", header = TRUE)

#library(dplyr)
#ag.people = read.csv("mp_people_properties_header_consol.csv", header = TRUE)
#joined <- left_join(ag.fico.groups, ag.people, by = c("distinct_id" = "distinct_id"), suffix = c("1", "2"))
#write.csv(joined, file = "fico_people.csv")

# Create the required subset
datadecision <- ag.decision.groups[,c(1:19)]
datadecision <- na.omit(datadecision)

datadecision$state <- as.numeric(datadecision$state)
datadecision$employment_status <- as.numeric(datadecision$employment_status)
```


```{r}
str(datadecision)
head(datadecision)
sapply(datadecision, class)
dim(datadecision)

g <- ggplot(datadecision, aes(fico_group))
g + geom_bar()
```

```{r}
str(datadecision)
datasub <- subset(datadecision, number_of_models_selected < 100)
f <- ggplot(datasub, aes(sessions, number_of_models_selected, fill = factor(fico_group)))
f + geom_jitter(aes(col = factor(fico_group)))

f <- ggplot(datadecision, aes(number_of_calculator_clicks, fill = factor(fico_group)))
f + geom_bar()

f <- ggplot(datadecision, aes(income, age, col = fico_group))
f + geom_jitter()

f <- ggplot(datadecision, aes(fico_group, sessions, col = age_group))
f + geom_jitter()

f <- ggplot(datadecision, aes(fico_group, sessions, col = fico_group))
f + geom_jitter()
```



```{r}
# Exercise 1: Remove the first column from the data and scale
# it using the scale() function
df1 <- scale(datadecision[-1])

# Now we'd like to cluster the data using K-Means. 
# How do we decide how many clusters to use if you don't know that already?
# We'll try two methods.

# Method 1: A plot of the total within-groups sums of squares against the 
# number of clusters in a K-means solution can be helpful. A bend in the 
# graph can suggest the appropriate number of clusters. 

wssplot <- function(datadecision, nc=15, seed=123) {
  wss <- (nrow(datadecision)-1)*sum(apply(datadecision,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(datadecision, centers=i)$withinss)}
  
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}

wssplot(df1)
```


```{r}
# Exercise 2:
#   * How many clusters does this method suggest?
#   * Why does this method work? What's the intuition behind it?
#   * Look at the code for wssplot() and figure out how it works

# Method 2: Use the NbClust library, which runs many experiments
# and gives a distribution of potential number of clusters.

library(NbClust)
set.seed(1234)
nc <- NbClust(df1, min.nc=2, max.nc=9, method="kmeans")
table(nc$Best.n[1,])

barplot(table(nc$Best.n[1,]),
        xlab="Numer of Clusters", ylab="Number of Criteria",
        main="Number of Clusters Chosen by Criteria")
```


```{r}
# Exercise 3: How many clusters does this method suggest?

# Exercise 4: Once you've picked the number of clusters, run k-means 
# using this number of clusters. Output the result of calling kmeans()
# into a variable fit.km
set.seed(43211)
fit.km1 <- kmeans(df1, 2, nstart=25)
fit.km1$size
```

```{r}
# Exercise 5: using the table() function, show how the clusters in fit.km$clusters
# compares to the actual wine types in wine$Type. Would you consider this a good
# clustering?
# Now we want to evaluate how well this clustering does.
fit.km1$centers 
fit.km1$cluster
```

```{r}
aggregate(datadecision[-1], by=list(cluster=fit.km1$cluster), mean)

ct.km1 <- table(datadecision$decision_status, fit.km1$cluster)
ct.km1  
```

```{r}
library(flexclust)
randIndex(ct.km1)
# Exercise 6:
# * Visualize these clusters using  function clusplot() from the cluster library
# * Would you consider this a good clustering?

clusplot(df1, fit.km1$cluster, color = TRUE, shade = TRUE)
fit.km1
```






```{r}
plot(datadecision[c("income", "number_of_models_selected")], col = fit.km1$cluster)
library(ggplot2)
f <- ggplot(datadecision, aes(fico_group, number_of_models_selected), col = fit.km$cluster)
f + geom_jitter(aes())

plot(datadecision[c("fico_group", "income")], col = fit.km1$cluster)
library(ggplot2)
f <- ggplot(datadecision, aes(fico_group, number_of_models_selected), col = fit.km$cluster)
f + geom_jitter(aes())

plot(datadecision[c("income", "number_of_models_selected")], col = fit.km1$cluster)
library(ggplot2)
f <- ggplot(datadecision, aes(fico_group, number_of_dealers_confirmed, col = fit.km1$cluster))
f + geom_jitter()

plot(datadecision[c("income", "number_of_models_selected")], col = fit.km1$cluster)
library(ggplot2)
f <- ggplot(datadecision, aes(fico_group, sessions, col = fit.km1$cluster))
f + geom_jitter()

plot(datadecision[c("income", "number_of_models_selected")], col = fit.km1$cluster)
library(ggplot2)
f <- ggplot(datadecision, aes(fico_group, number_of_calculator_clicks, col = fit.km1$cluster))
f + geom_jitter()














## Trying it out for fico_people
### Try the above for approved or declined.

# Now load the data and look at the first few rows
ficopeople = read.csv("/Users/Mandar/Documents/datascience/springboard_ag_capstone/clustering/fico_people_clean.csv", header = TRUE)

#library(dplyr)
#ag.people = read.csv("mp_people_properties_header_consol.csv", header = TRUE)
#joined <- left_join(ag.fico.groups, ag.people, by = c("distinct_id" = "distinct_id"), suffix = c("1", "2"))
#write.csv(joined, file = "fico_people.csv")

# Create the required subset
datadecision <- ficopeople[,c(1:10)]
View(datadecision)
datadecision <- subset(datadecision, income > 499)
datadecision <- subset(datadecision, number_of_models_selected > 0 & number_of_models_selected < 30)
datadecision <- subset(datadecision, number_of_lifetime_logins > 0 & number_of_lifetime_logins < 20)
datadecision <- subset(datadecision, number_of_years_selected > 0 & number_of_years_selected < 30)
datadecision <- subset(datadecision, number_of_trims_selected > 0 & number_of_trims_selected < 30)
datadecision <- subset(datadecision, number_of_trims_confirmed > 0 & number_of_trims_confirmed < 30)
datadecision <- subset(datadecision, number_of_dealers_selected > 0 & number_of_dealers_selected < 30)
datadecision <- subset(datadecision, number_of_dealers_confirmed > 0 & number_of_dealers_confirmed < 30)
datadecision <- subset(datadecision, number_of_dealers_confirmed > 0 & number_of_dealers_confirmed < 30)

# View(datadecision)
datadecision$state <- as.numeric(datadecision$state)
datadecision$employment_status <- as.numeric(datadecision$employment_status)

# Exercise 1: Remove the first column from the data and scale
# it using the scale() function
str(datadecision)
head(datadecision, 5)
sapply(datadecision, class)

df1 <- scale(datadecision[-1])
dim(datadecision)
datadecision = na.omit(datadecision)
dim(datadecision)

wssplot(df1)

# Now we'd like to cluster the data using K-Means. 
# How do we decide how many clusters to use if you don't know that already?
# We'll try two methods.

# Method 1: A plot of the total within-groups sums of squares against the 
# number of clusters in a K-means solution can be helpful. A bend in the 
# graph can suggest the appropriate number of clusters. 

wssplot <- function(datadecision, nc=10, seed=123) {
  wss <- (nrow(datadecision)-1)*sum(apply(datadecision,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(datadecision, centers=i)$withinss)}
  
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}

wssplot(df1)

# Exercise 2:
#   * How many clusters does this method suggest?
#   * Why does this method work? What's the intuition behind it?
#   * Look at the code for wssplot() and figure out how it works

# Method 2: Use the NbClust library, which runs many experiments
# and gives a distribution of potential number of clusters.

library(NbClust)
set.seed(1234)
nc <- NbClust(df1, min.nc=2, max.nc=7, method="kmeans")
table(nc$Best.n[1,])

barplot(table(nc$Best.n[1,]),
        xlab="Numer of Clusters", ylab="Number of Criteria",
        main="Number of Clusters Chosen by 26 Criteria")
```


```{r}
# Exercise 3: How many clusters does this method suggest?
 

# Exercise 4: Once you've picked the number of clusters, run k-means 
# using this number of clusters. Output the result of calling kmeans()
# into a variable fit.km

#set.seed(1234)
#fit.km1 <- kmeans(df1, 5, nstart=25)
#fit.km1$size
set.seed(1234)
fit.km1 <- kmeans(df1, 3, nstart=25)
fit.km1$size

# Now we want to evaluate how well this clustering does.
fit.km1$centers 

# Exercise 5: using the table() function, show how the clusters in fit.km$clusters
# compares to the actual wine types in wine$Type. Would you consider this a good
# clustering?
fit.km1$cluster
aggregate(datadecision[-1], by=list(cluster=fit.km1$cluster), mean)

ct.km1 <- table(datadecision$decision_status, fit.km1$cluster)
ct.km1
```

```{r}
library(flexclust)
randIndex(ct.km1)
# Exercise 6:
# * Visualize these clusters using  function clusplot() from the cluster library
# * Would you consider this a good clustering?

clusplot(df1, fit.km1$cluster, color = TRUE, shade = TRUE)
fit.km1

plot(datadecision[c("income", "number_of_trims_confirmed")], col = fit.km1$cluster)
```

```{r}
library(ggplot2)
str(datadecision)
```

```{r}
f <- ggplot(datadecision, aes(decision_status, income, col = factor(fit.km1$cluster)))
f + geom_jitter()
```

```{r}
# Wine

# Hierarchical clustering:
ficopeople = read.csv("/Users/Mandar/Documents/datascience/springboard_ag_capstone/clustering/fico_people_clean.csv", header = TRUE)
# Create the required subset
datadecision <- ficopeople[,c(1:10)]
datadecision <- na.omit(datadecision)
d <- dist(datadecision, method = "euclidean") # Euclidean distance matrix.
H.fit <- hclust(d, method="ward")

plot(H.fit)
groups <- cutree(H.fit, k=5)
rect.hclust(H.fit, k=5, border="red")

table(datadecision[,1],groups)
```

```{r}
# Study case III: Social Network Clustering Analysis
ficopeople = read.csv("/Users/Mandar/Documents/datascience/springboard_ag_capstone/clustering/fico_people_clean.csv", header = TRUE)
# Create the required subset
datadecision <- ficopeople[,c(1:12)]
dim(datadecision)
datadecision = na.omit(datadecision)
dim(datadecision)
datadecision_z <- as.data.frame(lapply(datadecision, scale))
user_clusters <- kmeans(datadecision_z, 5)
user_clusters$size
par(mfrow=c(2,2))
pie(colSums(datadecision[user_clusters$cluster==1,]),cex=0.5)
pie(colSums(datadecision[user_clusters$cluster==2,]),cex=0.5)
pie(colSums(datadecision[user_clusters$cluster==3,]),cex=0.5)
pie(colSums(datadecision[user_clusters$cluster==4,]),cex=0.5)
pie(colSums(datadecision[user_clusters$cluster==5,]),cex=0.5)
user_clusters$centers
```


```{r}
ficopeople_linear = read.csv("/Users/Mandar/Documents/datascience/springboard_ag_capstone/clustering/fico_people_clean.csv", header = TRUE)
# Separate into training and test
require(caTools)

ficogrp_linear <- ficopeople_linear[,c(1:14)]
ficogrp_linear <- na.omit(ficogrp_linear)
str(ficogrp_linear)

ficogrp_linear <- subset(ficogrp_linear, number_of_models_selected > 0 & number_of_models_selected < 30)
ficogrp_linear <- subset(ficogrp_linear, number_of_lifetime_logins > 0 & number_of_lifetime_logins < 20)
ficogrp_linear <- subset(ficogrp_linear, number_of_years_selected > 0 & number_of_years_selected < 30)
ficogrp_linear <- subset(ficogrp_linear, number_of_trims_selected > 0 & number_of_trims_selected < 30)
ficogrp_linear <- subset(ficogrp_linear, number_of_trims_confirmed > 0 & number_of_trims_confirmed < 30)
ficogrp_linear <- subset(ficogrp_linear, number_of_dealers_selected > 0 & number_of_dealers_selected < 30)
ficogrp_linear <- subset(ficogrp_linear, number_of_dealers_confirmed > 0 & number_of_dealers_confirmed < 30)
ficogrp_linear <- subset(ficogrp_linear, number_of_dealers_confirmed > 0 & number_of_dealers_confirmed < 30)

dim(ficogrp_linear)
str(ficogrp_linear)
# 75% of the sample size
smp_size <- floor(0.75 * nrow(ficogrp_linear))

# set the seed to make your partition reproductible
set.seed(54321)

train_ind.ficogrp <- sample(seq_len(nrow(ficogrp_linear)), size = smp_size)
train_fico.ficogrp <- ficogrp_linear[train_ind.ficogrp, ]
test_fico.ficogrp <- ficogrp_linear[-train_ind.ficogrp, ]

str(train_fico.ficogrp)
str(test_fico.ficogrp)

fico_grp_mod = lm(fico_group ~ number_of_dealers_selected + dp_by_loan + number_of_trims_selected, data = train_fico.ficogrp)
# fico_grp_mod = lm(fico_group ~ loan_amount + number_of_models_selected + number_of_calculator_clicks, data = train_fico.ficogrp)
summary(fico_grp_mod)

test_fico.ficogrp$fico_group <- predict(fico_grp_mod, newdata = test_fico.ficogrp)
write.csv(test_fico.ficogrp, file = "test_fico_ficogrp.csv")








```






## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
